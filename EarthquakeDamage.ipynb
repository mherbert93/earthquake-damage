{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((260601, 40), (86868, 39))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.merge(pd.read_csv('train_values.csv'),\n",
    "                 pd.read_csv('train_labels.csv'))\n",
    "test = pd.read_csv('test_values.csv')\n",
    "sample_submission = pd.read_csv('submission_format.csv')\n",
    "\n",
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#train, validation = train_test_split(train, train_size=0.80, test_size=0.20, stratify=train['damage_grade'], random_state=1337)\n",
    "\n",
    "#train.shape, validation.shape, test.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.568912\n3    0.334680\n1    0.096408\nName: damage_grade, dtype: float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['damage_grade'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    X = X.drop(['building_id'], axis=1)\n",
    "\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train = wrangle(train)\n",
    "#validation = wrangle(validation)\n",
    "test = wrangle(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train baseline accuracy is:  0.5689118614280068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "majority_class = train['damage_grade'].mode()[0]\n",
    "y_pred = [majority_class] * len(train['damage_grade'])\n",
    "\n",
    "print(\"Train baseline accuracy is: \", accuracy_score(train['damage_grade'], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# majority_class = validation['damage_grade'].mode()[0]\n",
    "# y_pred = [majority_class] * len(validation['damage_grade'])\n",
    "#\n",
    "# print(\"Validation baseline accuracy is: \", accuracy_score(validation['damage_grade'], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "target = 'damage_grade'\n",
    "\n",
    "train_features = train.drop([target], axis=1)\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "categorical_features = train_features.select_dtypes(exclude='number').nunique().index.tolist()\n",
    "\n",
    "\n",
    "features = numeric_features + categorical_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_train = train[target]\n",
    "X_train = train[features]\n",
    "# X_validation = validation[features]\n",
    "# y_validation = validation[target]\n",
    "\n",
    "X_test = test[features]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#When hyperparamater tuning, set tune to \"True\", and mark each model that we want to tune to \"True\".\n",
    "tune = False\n",
    "forest = True\n",
    "xgboost = False\n",
    "\n",
    "# forest_distributions = {\n",
    "#     'model__n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "#     'model__max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "#     'model__max_features': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
    "#                             31, 32, 33, 34, 35, 36, 37, 38],\n",
    "#     'model__min_samples_leaf': [2, 3, 4, 5, 6, 7, 8]\n",
    "# }\n",
    "#\n",
    "# xgboost_distributions = {\n",
    "#     'model__n_estimators': [50, 60, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300],\n",
    "#     'model__max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "#     'model__learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20, 0.22, 0.24],\n",
    "#     'model__min_child_leaf':[1, 2, 3, 4, 5, 6],\n",
    "#     'model__min_child_weight': [1, 2, 3, 4, 5],\n",
    "#     'model__colsample_bytree':[0.1, 0.15, 0.2, 0.3, 0.4, 0.50, 0.60, 0.70],\n",
    "#     'model__subsample':[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "#     'model__gamma':[0, 1, 2, 3, 4, 5, 10, 20],\n",
    "#     'model__scale_pos_weight': [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70,\n",
    "#                                 75, 80, 85, 90, 95, 100]\n",
    "#  }\n",
    "\n",
    "forest_distributions = {\n",
    "    'model__n_estimators': [350, 375, 400, 425],\n",
    "    'model__max_depth': [29, 30, 31, 32, 33],\n",
    "    'model__max_features': [24, 25, 26, 27],\n",
    "    'model__min_samples_leaf': [4]\n",
    "}\n",
    "\n",
    "xgboost_distributions = {\n",
    "    'model__n_estimators': [260, 265, 270],\n",
    "    'model__max_depth': [26, 27],\n",
    "    'model__learning_rate': [0.08, 0.09],\n",
    "    'model__min_child_leaf':[3],\n",
    "    'model__min_child_weight': [4],\n",
    "    'model__colsample_bytree':[0.2],\n",
    "    'model__subsample':[0.9, 1],\n",
    "    'model__gamma':[0],\n",
    "    'model__scale_pos_weight': [25]\n",
    "  }\n",
    "\n",
    "if tune: #If we are hyperparamater tuning, pass no parameters into estimators and find paramaters via GridSearchCV / RandomizedSearchCV\n",
    "\n",
    "    forest_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                           ('model', RandomForestClassifier(random_state=1337))])\n",
    "\n",
    "    xgboost_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                           ('model', XGBClassifier(seed=1337))])\n",
    "\n",
    "    # forest_search = BayesSearchCV(\n",
    "    #     forest_pipeline,\n",
    "    #     search_spaces=forest_distributions,\n",
    "    #     n_iter=100,\n",
    "    #     cv=3,\n",
    "    #     scoring='f1_micro',\n",
    "    #     random_state=1337,\n",
    "    #     n_points=5,\n",
    "    #     n_jobs=15\n",
    "    # )\n",
    "    # xgboost_search = BayesSearchCV(\n",
    "    #     estimator=xgboost_pipeline,\n",
    "    #     search_spaces=xgboost_distributions,\n",
    "    #     n_iter=800,\n",
    "    #     cv=3,\n",
    "    #     scoring='f1_micro',\n",
    "    #     random_state=1337,\n",
    "    #     n_points=5,\n",
    "    #     n_jobs=15\n",
    "    # )\n",
    "\n",
    "    forest_search = GridSearchCV(\n",
    "        estimator=forest_pipeline,\n",
    "        param_grid=forest_distributions,\n",
    "        cv=3,\n",
    "        verbose=3,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=15\n",
    "    )\n",
    "    xgboost_search = GridSearchCV(\n",
    "        estimator=xgboost_pipeline,\n",
    "        param_grid=xgboost_distributions,\n",
    "        cv=3,\n",
    "        verbose=3,\n",
    "        scoring='f1_micro',\n",
    "        n_jobs=15\n",
    "    )\n",
    "\n",
    "    def report_step_forest(optim_result):\n",
    "        score = forest_search.best_score_\n",
    "        print(\"Random Forest best score: %s\" % score)\n",
    "\n",
    "    def report_step_xgboost(optim_result):\n",
    "        score = xgboost_search.best_score_\n",
    "        print(\"XGboost best score: %s\" % score)\n",
    "\n",
    "    #X_train, y_train = RandomOverSampler(sampling_strategy='not majority').fit_resample(X_train, y_train)\n",
    "\n",
    "    if forest:\n",
    "        forest_search.fit(X_train, y_train)\n",
    "        forest_train_pred = forest_search.predict(X_train)\n",
    "        forest_validation_pred = forest_search.predict(X_validation)\n",
    "        forest_validation_pred_proba = forest_search.predict_proba(X_validation)\n",
    "\n",
    "    if xgboost:\n",
    "        xgboost_search.fit(X_train, y_train)\n",
    "        xgboost_train_pred = xgboost_search.predict(X_train)\n",
    "        xgboost_validation_pred = xgboost_search.predict(X_validation)\n",
    "        xgboost_validation_pred_proba = xgboost_search.predict_proba(X_validation)\n",
    "\n",
    "    #When hyperparameter tuning, pass our best estimators into votingclassifier. Only run when all 3 models are being tuned.\n",
    "    if forest and xgboost:\n",
    "        voting_model = VotingClassifier(estimators=[('forest', forest_search.best_estimator_), #VotingClassifier is Soft Voting/Majority Rule classifier for unfitted estimators\n",
    "                                                    ('xgboost', xgboost_search.best_estimator_),],\n",
    "                                        voting='soft') #soft voting per recommendation from sklearn documentation, when used on tuned classifiers.\n",
    "        voting_model.fit(X_train, y_train)\n",
    "        # combined_model_pred = voting_model.predict(X_validation)\n",
    "\n",
    "else: #If we are not hyperparameter tuning, pass in our best params(from previous tuning runs).\n",
    "    forest_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                            ('model', RandomForestClassifier(random_state=1337,\n",
    "                                                             n_jobs=15, max_depth=30, max_features=25, min_samples_leaf=4,\n",
    "                                                             n_estimators=350))])\n",
    "\n",
    "    xgboost_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                                ('model', XGBClassifier(seed=1337, n_jobs=15, colsample_bytree=0.2, gamma=0,\n",
    "                                                        learning_rate=0.09, max_depth=26, min_child_leaf=3, min_child_weight=4,\n",
    "                                                        n_estimators=260, scale_pos_weight=25, subsample=0.9))])\n",
    "\n",
    "\n",
    "    #X_train, y_train = RandomOverSampler(sampling_strategy='minority').fit_resample(X_train, y_train) #over sample all but the majority class\n",
    "\n",
    "    forest_pipeline.fit(X_train, y_train)\n",
    "    forest_train_pred = forest_pipeline.predict(X_train)\n",
    "    # forest_validation_pred = forest_pipeline.predict(X_validation)\n",
    "    # forest_validation_pred_proba = forest_pipeline.predict_proba(X_validation)\n",
    "    forest_test_pred = forest_pipeline.predict(X_test)\n",
    "\n",
    "    xgboost_pipeline.fit(X_train, y_train)\n",
    "    xgboost_train_pred = xgboost_pipeline.predict(X_train)\n",
    "    # xgboost_validation_pred = xgboost_pipeline.predict(X_validation)\n",
    "    # xgboost_validation_pred_proba = xgboost_pipeline.predict_proba(X_validation)\n",
    "    xgboost_test_pred = xgboost_pipeline.predict(X_test)\n",
    "\n",
    "    voting_model = VotingClassifier(estimators=[('forest', forest_pipeline), #VotingClassifier is Soft Voting/Majority Rule classifier for unfitted estimators\n",
    "                                                ('xgboost', xgboost_pipeline),],\n",
    "                                    voting='soft') #soft voting per recommendation from sklearn documentation, when used on tuned classifiers.\n",
    "    voting_model.fit(X_train, y_train)\n",
    "    # combined_model_pred = voting_model.predict(X_validation)\n",
    "    combined_model_test_pred = voting_model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, classification_report\n",
    "# if tune:\n",
    "#\n",
    "#     if forest:\n",
    "#         print(forest_search.best_params_, '\\n')\n",
    "#         print(\"Best Random Forest CV score: \", forest_search.best_score_, '\\n')\n",
    "#         print(\"Random forest F1-Micro: \", f1_score(y_validation, forest_validation_pred, average='micro'), '\\n')\n",
    "#         print(classification_report(y_validation, forest_validation_pred), '\\n')\n",
    "#\n",
    "#         print('\\n')\n",
    "#\n",
    "#     if xgboost:\n",
    "#         print(xgboost_search.best_params_, '\\n')\n",
    "#         print(\"Best xgboost CV score: \", xgboost_search.best_score_, '\\n')\n",
    "#         print(\"Xgboost F1-Micro: \", f1_score(y_validation, xgboost_validation_pred, average='micro'), '\\n')\n",
    "#         print(classification_report(y_validation, xgboost_validation_pred), '\\n')\n",
    "#\n",
    "#         print('\\n')\n",
    "#\n",
    "#     if forest and xgboost:\n",
    "#         print(\"Voting classifier, final F1-Micro score on validation set: \", f1_score(y_validation, combined_model_pred, average='micro'))\n",
    "# else:\n",
    "#     print(\"Random forest F1-Micro: \", f1_score(y_validation, forest_validation_pred, average='micro'), '\\n')\n",
    "#     print(classification_report(y_validation, forest_validation_pred))\n",
    "#\n",
    "#     print('\\n')\n",
    "#\n",
    "#     print(\"Xgboost F1-Micro: \", f1_score(y_validation, xgboost_validation_pred, average='micro'), '\\n')\n",
    "#     print(classification_report(y_validation, xgboost_validation_pred))\n",
    "#\n",
    "#     print('\\n')\n",
    "#\n",
    "#     print(\"Voting classifier, final F1-Micro score on validation set: \", f1_score(y_validation, combined_model_pred, average='micro'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
    "#\n",
    "# profile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# #combined_model_test_pred = voting_model.predict(X_test)\n",
    "#xgboost_test_pred = xgboost_search.predict(X_test)\n",
    "submission = sample_submission.copy()\n",
    "submission['damage_grade'] = xgboost_test_pred\n",
    "submission.to_csv('xgboost.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    xgboost_pipeline.named_steps.model, #prefit estimator\n",
    "    scoring='f1_micro',\n",
    "    n_iter=5,\n",
    "    random_state=1337\n",
    ")\n",
    "\n",
    "permuter.fit(forest_pipeline.named_steps.encoder.transform(X_validation), y_validation)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_names = X_validation.columns.tolist()\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eli5.show_weights(\n",
    "    permuter,\n",
    "    top=None,\n",
    "    feature_names=feature_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}