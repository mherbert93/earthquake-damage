{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((260601, 40), (86868, 39))"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.merge(pd.read_csv('train_values.csv'),\n",
    "                 pd.read_csv('train_labels.csv'))\n",
    "test = pd.read_csv('test_values.csv')\n",
    "sample_submission = pd.read_csv('submission_format.csv')\n",
    "\n",
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "#train, validation = train_test_split(train, train_size=0.80, test_size=0.20, stratify=train['damage_grade'], random_state=1337)\n",
    "\n",
    "#train.shape, validation.shape, test.shape\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "data": {
      "text/plain": "2    0.568912\n3    0.334680\n1    0.096408\nName: damage_grade, dtype: float64"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['damage_grade'].value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "\n",
    "    X = X.drop(['building_id'], axis=1)\n",
    "\n",
    "    # X.iloc[:, 14:25] = X.iloc[:, 14:25].astype(str)\n",
    "    # X.iloc[:, 27:38] = X.iloc[27:38].astype(str)\n",
    "\n",
    "    # X[['has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
    "    #    'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick',\n",
    "    #    'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo',\n",
    "    #    'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
    "    #    'has_secondary_use', 'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
    "    #    'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school',\n",
    "    #    'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
    "    #    'has_secondary_use_use_police', 'has_secondary_use_other']] = X[['has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n",
    "    #    'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick',\n",
    "    #    'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo',\n",
    "    #    'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other',\n",
    "    #    'has_secondary_use', 'has_secondary_use_agriculture', 'has_secondary_use_hotel',\n",
    "    #    'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school',\n",
    "    #    'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office',\n",
    "    #    'has_secondary_use_use_police', 'has_secondary_use_other']].astype(str)\n",
    "\n",
    "    # X.iloc[:, 14:25] = X.iloc[:, 14:25].astype(str)\n",
    "    # X.iloc[:, 27:38] = X.iloc[27:38].astype(str)\n",
    "\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [],
   "source": [
    "train = wrangle(train)\n",
    "#validation = wrangle(validation)\n",
    "test = wrangle(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "geo_level_1_id                             int64\ngeo_level_2_id                             int64\ngeo_level_3_id                             int64\ncount_floors_pre_eq                        int64\nage                                        int64\narea_percentage                            int64\nheight_percentage                          int64\nland_surface_condition                    object\nfoundation_type                           object\nroof_type                                 object\nground_floor_type                         object\nother_floor_type                          object\nposition                                  object\nplan_configuration                        object\nhas_superstructure_adobe_mud               int64\nhas_superstructure_mud_mortar_stone        int64\nhas_superstructure_stone_flag              int64\nhas_superstructure_cement_mortar_stone     int64\nhas_superstructure_mud_mortar_brick        int64\nhas_superstructure_cement_mortar_brick     int64\nhas_superstructure_timber                  int64\nhas_superstructure_bamboo                  int64\nhas_superstructure_rc_non_engineered       int64\nhas_superstructure_rc_engineered           int64\nhas_superstructure_other                   int64\nlegal_ownership_status                    object\ncount_families                             int64\nhas_secondary_use                          int64\nhas_secondary_use_agriculture              int64\nhas_secondary_use_hotel                    int64\nhas_secondary_use_rental                   int64\nhas_secondary_use_institution              int64\nhas_secondary_use_school                   int64\nhas_secondary_use_industry                 int64\nhas_secondary_use_health_post              int64\nhas_secondary_use_gov_office               int64\nhas_secondary_use_use_police               int64\nhas_secondary_use_other                    int64\ndamage_grade                               int64\ndtype: object"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train baseline accuracy is:  0.5689118614280068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "majority_class = train['damage_grade'].mode()[0]\n",
    "y_pred = [majority_class] * len(train['damage_grade'])\n",
    "\n",
    "print(\"Train baseline accuracy is: \", accuracy_score(train['damage_grade'], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "target = 'damage_grade'\n",
    "\n",
    "train_features = train.drop([target], axis=1)\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "categorical_features = train_features.select_dtypes(exclude='number').nunique().index.tolist()\n",
    "\n",
    "\n",
    "features = numeric_features + categorical_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "# import category_encoders as ce\n",
    "#\n",
    "# encoder = ce.OneHotEncoder()\n",
    "# X_train_encoded = encoder.fit_transform(train[categorical_features])\n",
    "# X_test_encoded = encoder.transform(test[categorical_features])\n",
    "#\n",
    "# X_train_numeric = train[numeric_features]\n",
    "# X_test_numeric = test[numeric_features]\n",
    "#\n",
    "# X_train_nn = pd.concat([X_train_encoded, X_test_numeric], axis=1)\n",
    "# X_test_nn = pd.concat([X_test_encoded, X_test_numeric], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "y_train = train[target]\n",
    "X_train = train[features]\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)\n",
    "\n",
    "X_test = test[features]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "#\n",
    "# scaler = MinMaxScaler()\n",
    "#\n",
    "# X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "# X_test_scaled = scaler.transform(X_test_encoded)\n",
    "#\n",
    "# y_train_cat = to_categorical(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [
    {
     "data": {
      "text/plain": "(260601, 38)"
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:12] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { min_child_leaf, scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#When hyperparamater tuning, set tune to \"True\", and mark each model that we want to tune to \"True\".\n",
    "tune = False\n",
    "forest = False\n",
    "xgboost = False\n",
    "extratrees = False\n",
    "neuralnetwork = False\n",
    "\n",
    "forest_distributions = {\n",
    "    'model__n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'model__max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "    'model__max_features': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
    "                            31, 32, 33, 34, 35, 36, 37, 38],\n",
    "    'model__min_samples_leaf': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# xgboost_distributions = {\n",
    "#     'model__n_estimators': [125, 150, 175, 200, 225, 250, 260, 275, 300],\n",
    "#     'model__max_depth': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "#     'model__learning_rate': [0.03, 0.04, 0.05, 0.07, 0.09, 0.10, 0.12, 0.14, 0.16],\n",
    "#     'model__min_child_leaf':[1, 2, 3, 4, 5],\n",
    "#     'model__min_child_weight': [1, 2, 3, 4, 5],\n",
    "#     'model__colsample_bytree':[0.1, 0.15, 0.2, 0.3, 0.4, 0.50],\n",
    "#     'model__subsample':[0.6, 0.7, 0.8, 0.9, 1],\n",
    "#     'model__gamma':[0, 1, 2, 3, 4, 5],\n",
    "#     'model__scale_pos_weight': [20, 25, 30, 35, 40, 45, 50]\n",
    "#  }\n",
    "\n",
    "xgboost_distributions = {\n",
    "    'model__n_estimators': [270, 275, 280],\n",
    "    'model__max_depth': [26],\n",
    "    'model__learning_rate': [0.08],\n",
    "    'model__min_child_leaf':[2, 3],\n",
    "    'model__min_child_weight': [1, 2],\n",
    "    'model__colsample_bytree':[0.19, 0.2, 0.21],\n",
    "    'model__subsample':[0.93, 0.94, 0.95, 0.96],\n",
    "    'model__gamma':[0.9, 1, 1.1],\n",
    "    'model__scale_pos_weight': [35, 36, 37, 38]\n",
    " }\n",
    "\n",
    "extratrees_distributions = {\n",
    "    'model__n_estimators': [50, 100, 150, 200, 250, 300, 325, 350, 375, 400, 425],\n",
    "    'model__max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
    "    'model__min_samples_leaf': [2, 3, 4, 5, 6, 7, 8],\n",
    "    'model__max_features': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
    "                            31, 32, 33, 34, 35, 36, 37, 38],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__bootstrap': [True, False],\n",
    "    'model__class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# forest_distributions = {\n",
    "#     'model__n_estimators': [350, 375, 400, 425],\n",
    "#     'model__max_depth': [29, 30, 31, 32, 33],\n",
    "#     'model__max_features': [24, 25, 26, 27],\n",
    "#     'model__min_samples_leaf': [4]\n",
    "# }\n",
    "\n",
    "# xgboost_distributions = {\n",
    "#     'model__n_estimators': [260, 265, 270],\n",
    "#     'model__max_depth': [26, 27],\n",
    "#     'model__learning_rate': [0.08, 0.09],\n",
    "#     'model__min_child_leaf':[3],\n",
    "#     'model__min_child_weight': [4],\n",
    "#     'model__colsample_bytree':[0.2],\n",
    "#     'model__subsample':[0.9, 1],\n",
    "#     'model__gamma':[0],\n",
    "#     'model__scale_pos_weight': [25]\n",
    "#   }\n",
    "\n",
    "if tune: #If we are hyperparamater tuning, pass no parameters into estimators and find paramaters via GridSearchCV / RandomizedSearchCV\n",
    "\n",
    "    forest_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                           ('model', RandomForestClassifier(random_state=1337))])\n",
    "\n",
    "    xgboost_pipeline = Pipeline([('encoder', ce.TargetEncoder()),\n",
    "                           ('model', XGBClassifier(seed=1337))])\n",
    "\n",
    "    extratrees_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                                ('model', ExtraTreesClassifier(random_state=1337))])\n",
    "\n",
    "\n",
    "    forest_search = BayesSearchCV(\n",
    "        forest_pipeline,\n",
    "        search_spaces=forest_distributions,\n",
    "        n_iter=500,\n",
    "        cv=3,\n",
    "        scoring='f1_micro',\n",
    "        random_state=1337,\n",
    "        n_points=5,\n",
    "        n_jobs=15\n",
    "    )\n",
    "    xgboost_search = BayesSearchCV(\n",
    "        estimator=xgboost_pipeline,\n",
    "        search_spaces=xgboost_distributions,\n",
    "        n_iter=500,\n",
    "        cv=3,\n",
    "        scoring='f1_micro',\n",
    "        random_state=1337,\n",
    "        n_points=5,\n",
    "        n_jobs=15\n",
    "    )\n",
    "    extratrees_search = BayesSearchCV(\n",
    "        extratrees_pipeline,\n",
    "        search_spaces=extratrees_distributions,\n",
    "        n_iter=500,\n",
    "        cv=3,\n",
    "        scoring='f1_micro',\n",
    "        random_state=1337,\n",
    "        n_points=5,\n",
    "        n_jobs=15\n",
    "    )\n",
    "\n",
    "    # forest_search = GridSearchCV(\n",
    "    #     estimator=forest_pipeline,\n",
    "    #     param_grid=forest_distributions,\n",
    "    #     cv=3,\n",
    "    #     verbose=3,\n",
    "    #     scoring='f1_micro',\n",
    "    #     n_jobs=15\n",
    "    # )\n",
    "    # xgboost_search = GridSearchCV(\n",
    "    #     estimator=xgboost_pipeline,\n",
    "    #     param_grid=xgboost_distributions,\n",
    "    #     cv=3,\n",
    "    #     verbose=3,\n",
    "    #     scoring='f1_micro',\n",
    "    #     n_jobs=15\n",
    "    # )\n",
    "\n",
    "    def report_step_forest(optim_result):\n",
    "        global previous_score\n",
    "        global i\n",
    "        global total_i\n",
    "        score = forest_search.best_score_\n",
    "        print(\"Random Forest best score: %s\" % score)\n",
    "        if score > previous_score:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "        print(\"Iterations since improvement: \", i)\n",
    "        total_i += 1\n",
    "        print(\"Total iterations: \", total_i)\n",
    "        if i > 6:\n",
    "            print(\"Over 6 iterations with no improvement, aborting...\")\n",
    "            return True\n",
    "        previous_score = score\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    def report_step_extratrees(optim_result):\n",
    "        global previous_score\n",
    "        global i\n",
    "        global total_i\n",
    "        score = extratrees_search.best_score_\n",
    "        print(\"Extra trees best score: %s\" % score)\n",
    "        if score > previous_score:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "        print(\"Iterations since improvement: \", i)\n",
    "        total_i += 1\n",
    "        print(\"Total iterations: \", total_i)\n",
    "        if i > 6:\n",
    "            print(\"Over 6 iterations with no improvement, aborting...\")\n",
    "            return True\n",
    "        previous_score = score\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    def report_step_xgboost(optim_result):\n",
    "        global previous_score\n",
    "        global i\n",
    "        global total_i\n",
    "        score = xgboost_search.best_score_\n",
    "        print(\"XGboost best score: %s\" % score)\n",
    "        if score > previous_score:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "        print(\"Iterations since improvement: \", i)\n",
    "        total_i += 1\n",
    "        print(\"Total iterations: \", total_i)\n",
    "        if i > 5:\n",
    "            print(\"Over 5 iterations with no improvement, aborting...\")\n",
    "            return True\n",
    "        previous_score = score\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "    #X_train, y_train = RandomOverSampler(sampling_strategy='not majority').fit_resample(X_train, y_train)\n",
    "\n",
    "    if forest:\n",
    "        i = 0\n",
    "        total_i = 0\n",
    "        previous_score = 0\n",
    "\n",
    "        forest_search.fit(X_train, y_train, callback=report_step_forest)\n",
    "        forest_train_pred = forest_search.predict(X_train)\n",
    "        forest_test_pred = forest_search.predict(X_test)\n",
    "\n",
    "    if xgboost:\n",
    "        i = 0\n",
    "        total_i = 0\n",
    "        previous_score = 0\n",
    "\n",
    "        xgboost_search.fit(X_train, y_train, callback=report_step_xgboost)\n",
    "        xgboost_train_pred = xgboost_search.predict(X_train)\n",
    "        xgboost_test_pred = xgboost_search.predict(X_test)\n",
    "\n",
    "    if extratrees:\n",
    "        i = 0\n",
    "        total_i = 0\n",
    "        previous_score = 0\n",
    "\n",
    "        extratrees_search.fit(X_train, y_train, callback=report_step_extratrees)\n",
    "        extratrees_train_pred_proba = extratrees_search.predict_proba(X_train)\n",
    "        extratrees_test_pred_proba = extratrees_search.predict_proba(X_test)\n",
    "\n",
    "    # if neuralnetwork:\n",
    "    #     def keras_model():\n",
    "    #         network = Sequential()\n",
    "    #\n",
    "    #         network.add(Dense(90, activation='relu'))\n",
    "    #         network.add(Dense(512, activation='relu'))\n",
    "    #         network.add(Dropout(0.2))\n",
    "    #         network.add(Dense(256, activation='relu'))\n",
    "    #         network.add(Dropout(0.2))\n",
    "    #         network.add(Dense(128, activation='relu'))\n",
    "    #         network.add(Dropout(0.1))\n",
    "    #         network.add(Dense(3, activation='softmax'))\n",
    "    #\n",
    "    #         network.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    #\n",
    "    #         return network\n",
    "    #\n",
    "    #     early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    #\n",
    "    #     # nn.fit(x=X_train_scaled, y=y_train_cat, epochs=9999, validation_split=0.2, callbacks=[early_stop])\n",
    "    #\n",
    "    #     nn = KerasClassifier(build_fn=keras_model, epochs=10, batch_size=32,\n",
    "    #                          validation_split=0.2)\n",
    "\n",
    "        # nn.fit(X_train_scaled, y_train_cat)\n",
    "\n",
    "    #When hyperparameter tuning, pass our best estimators into votingclassifier. Only run when all 3 models are being tuned.\n",
    "    # if forest and xgboost:\n",
    "    #     voting_model = VotingClassifier(estimators=[('forest', forest_search.best_estimator_), #VotingClassifier is Soft Voting/Majority Rule classifier for unfitted estimators\n",
    "    #                                                 ('xgboost', xgboost_search.best_estimator_),\n",
    "    #                                                 ('extratrees', extratrees_search.best_estimator_),],\n",
    "    #                                     voting='hard') #soft voting per recommendation from sklearn documentation, when used on tuned classifiers.\n",
    "    #     voting_model_cv = voting_model\n",
    "    #     voting_model.fit(X_train, y_train)\n",
    "    #     combined_model_test_pred = voting_model.predict(X_test)\n",
    "\n",
    "else: #If we are not hyperparameter tuning, pass in our best params(from previous tuning runs).\n",
    "    forest_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                            ('model', RandomForestClassifier(random_state=1337,\n",
    "                                                             n_jobs=12, max_depth=30, max_features=25, min_samples_leaf=4,\n",
    "                                                             n_estimators=350))])\n",
    "\n",
    "    xgboost_pipeline = Pipeline([('encoder', ce.TargetEncoder()),\n",
    "                                ('model', XGBClassifier(seed=1337, n_jobs=12, colsample_bytree=0.2, gamma=1,\n",
    "                                                        learning_rate=0.08, max_depth=26, min_child_leaf=2, min_child_weight=1,\n",
    "                                                        n_estimators=265, scale_pos_weight=37, subsample=0.94))])\n",
    "\n",
    "    forest_entropy_pipeline = Pipeline([('encoder', ce.OrdinalEncoder()),\n",
    "                                ('model', RandomForestClassifier(random_state=1337, n_jobs=12, max_depth=28,\n",
    "                                                             min_samples_leaf=2, max_features=26, n_estimators=200,\n",
    "                                                              criterion='entropy'))])\n",
    "\n",
    "\n",
    "    #X_train, y_train = RandomOverSampler(sampling_strategy='minority').fit_resample(X_train, y_train) #over sample all but the majority class\n",
    "\n",
    "    forest_pipeline_cv = forest_pipeline\n",
    "    forest_pipeline.fit(X_train, y_train)\n",
    "    forest_train_pred = forest_pipeline.predict(X_train)\n",
    "    forest_test_pred = forest_pipeline.predict(X_test)\n",
    "\n",
    "    xgboost_pipeline_cv = xgboost_pipeline\n",
    "    xgboost_pipeline.fit(X_train, y_train)\n",
    "    xgboost_train_pred = xgboost_pipeline.predict(X_train)\n",
    "    xgboost_test_pred = xgboost_pipeline.predict(X_test)\n",
    "\n",
    "    forest_entropy_pipeline_cv = forest_entropy_pipeline\n",
    "    forest_entropy_pipeline.fit(X_train, y_train)\n",
    "    forest_entropy__train_pred_proba = forest_entropy_pipeline.predict_proba(X_train)\n",
    "    forest_entropy_test_pred_proba = forest_entropy_pipeline.predict_proba(X_test)\n",
    "\n",
    "    #\n",
    "    # voting_model = VotingClassifier(estimators=[('forest', forest_pipeline),\n",
    "    #                                             ('xgboost', xgboost_pipeline),\n",
    "    #                                             ('extratrees', extratrees_pipeline),\n",
    "    #                                             ('extratrees_entropy', extratrees_entropy_pipeline),\n",
    "    #                                             ('forest_entropy', forest_entropy_pipeline),],\n",
    "    #                                 voting='hard', weights=[1, 3, 1, 1, 1], n_jobs=15)\n",
    "    # voting_model_cv = voting_model\n",
    "    # voting_model.fit(X_train, y_train)\n",
    "    # combined_model_test_pred = voting_model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost F1-Micro:  0.7515780829697506 \n",
      "\n",
      "\n",
      "\n",
      "Random forest gini(class balance=None) F1-Micro:  0.7395136626490305 \n",
      "\n",
      "\n",
      "\n",
      "Random forest entropy F1-Micro:  0.7363018560941823 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "if tune:\n",
    "    if forest:\n",
    "        print(forest_search.best_params_, '\\n')\n",
    "        print(\"Best Random Forest CV score: \", forest_search.best_score_, '\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    if xgboost:\n",
    "        print(xgboost_search.best_params_, '\\n')\n",
    "        print(\"Best xgboost CV score: \", xgboost_search.best_score_, '\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    if extratrees:\n",
    "        print(extratrees_search.best_params_, '\\n')\n",
    "        print(\"Best Extra Trees CV score: \", extratrees_search.best_score_, '\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    # if neuralnetwork:\n",
    "    #     print(\"Neural network CV score:\", cross_val_score(nn, X_train_scaled, y_train,\n",
    "    #                                                         scoring='f1_micro', cv=3, n_jobs=1))\n",
    "\n",
    "    # if forest and xgboost and extratrees:\n",
    "    #     #print(\"Voting classifier, final F1-Micro score on validation set: \", f1_score(y_validation, combined_model_pred, average='micro'))\n",
    "    #     print(\"Voting classifier, final F1-Micro cross-validation score: \", np.mean(cross_val_score(voting_model_cv, X_train,\n",
    "    #                                                                                         y_train, scoring='f1_micro',\n",
    "    #                                                                                         cv=3, n_jobs=15)))\n",
    "else:\n",
    "\n",
    "    print(\"Xgboost F1-Micro: \", np.mean(cross_val_score(xgboost_pipeline_cv, X_train, y_train, scoring='f1_micro', cv=3,\n",
    "                                                n_jobs=12)), '\\n')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"Random forest gini(class balance=None) F1-Micro: \", np.mean(cross_val_score(forest_pipeline_cv, X_train, y_train, scoring='f1_micro', cv=3,\n",
    "                                                n_jobs=12)), '\\n')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"Random forest entropy F1-Micro: \", np.mean(cross_val_score(forest_entropy_pipeline_cv, X_train, y_train, scoring='f1_micro', cv=3,\n",
    "                                                n_jobs=12)), '\\n')\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    #print(\"Xgboost F1-Micro training score: \", f1_score(y_train, xgboost_train_pred, average='micro'))\n",
    "\n",
    "    # print(\"Voting classifier, final voting ensemble cross-validation score: \", np.mean(cross_val_score(voting_model_cv, X_train,\n",
    "    #                                                                                         y_train, scoring='f1_micro',\n",
    "    #                                                                                         cv=3, n_jobs=15)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
    "#\n",
    "# profile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()\n",
    "submission['damage_grade'] = xgboost_test_pred\n",
    "submission.to_csv('xgboost.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "#\n",
    "# encoder = ce.OrdinalEncoder()\n",
    "#\n",
    "#\n",
    "# x_train_encoded = encoder.fit_transform(X_train)\n",
    "#\n",
    "# model = XGBClassifier(seed=1337, n_jobs=15, colsample_bytree=0.2, gamma=0,\n",
    "#                                                         learning_rate=0.09, max_depth=26, min_child_leaf=3, min_child_weight=4,\n",
    "#                                                         n_estimators=260, scale_pos_weight=25, subsample=0.9)\n",
    "#\n",
    "#\n",
    "#\n",
    "# feature_selector = RFECV(model, n_jobs=15, cv=3, scoring='f1_micro', step=2)\n",
    "# feature_selector.fit(x_train_encoded, y_train)\n",
    "#\n",
    "# features_mask = feature_selector.support_\n",
    "# features_ranking = feature_selector.ranking_\n",
    "#\n",
    "# print(feature_selector.n_features_)\n",
    "\n",
    "#print(\"New F1-Score: \", f1_score())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if tune:\n",
    "    model = forest_search.best_estimator_.named_steps.model\n",
    "    encoder = forest_search.best_estimator_.named_steps.encoder\n",
    "else:\n",
    "    model = xgboost_pipeline.named_steps.model\n",
    "    encoder = xgboost_pipeline.named_steps.encoder\n",
    "\n",
    "\n",
    "encoded_columns = encoder.transform(X_test).columns\n",
    "importances = pd.Series(model.feature_importances_, encoded_columns)\n",
    "plt.figure(figsize=(10,30))\n",
    "importances.sort_values().plot.barh(color='grey');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "unit4-sprint2",
   "language": "python",
   "display_name": "Unit4-Sprint2 (Python3)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}